{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/Projects/spatial_dist_of_music/yelp_data_work/llm_work/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file does the finetuning work necessary for the classification of\n",
    "Yelp reviews that do or do not mention that the place they are for offers\n",
    "live music.\n",
    "\n",
    "NOTE: Everything here is highly based on Stephen Hansen's GitHub tutorial\n",
    "at https://github.com/sekhansen/columbia_lectures_2025/blob/main/code/03_classification_bert.ipynb\n",
    "and his paper REMOTE WORK ACROSS JOBS, COMPANIES, AND SPACE (Hansen et al.)\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12923ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed at the beginning\n",
    "set_seed(42)\n",
    "\n",
    "# setup running on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using Apple Metal Performance Shaders (MPS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38711869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"./finetuning_reviews.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f779a",
   "metadata": {},
   "source": [
    "Prepare data for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3194712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train-test split\n",
    "\n",
    "n = len(df)\n",
    "test_size = int(0.1 * n)\n",
    "indices = np.random.RandomState(95).permutation(n)\n",
    "train_idxs, test_idxs = indices[test_size:], indices[:test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune = df.iloc[train_idxs][[\"text\", \"live_music\"]].copy()\n",
    "df_finetune['labels'] = df_finetune['live_music']\n",
    "print(df_finetune.shape)\n",
    "\n",
    "df_test = df.iloc[test_idxs][[\"text\", \"live_music\"]].copy()\n",
    "df_test['labels'] = df_test['live_music']\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3018b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finetune.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca02f09",
   "metadata": {},
   "source": [
    "Finetune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into Dataset class\n",
    "finetune_dataset = Dataset.from_pandas(df_finetune)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311262ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use cased to identify proper nouns\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=512, padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# batched=True is key for training\n",
    "tokenized_ft = finetune_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\",                # path to save model\n",
    "    learning_rate=5e-5,             # small learning rates\n",
    "    num_train_epochs=2,             # number of finetuning passes\n",
    "    per_device_train_batch_size=8,  # batch size per GPU\n",
    "    per_device_eval_batch_size=8,   # batch size per GPU\n",
    "    eval_strategy=\"epoch\",          # See metrics during training\n",
    "    save_strategy=\"no\",             # Don't save checkpoints\n",
    "    report_to=\"none\",               # Don't report model estimation externally\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance metrics\n",
    "\n",
    "metric_precision = evaluate.load(\"precision\")\n",
    "metric_recall = evaluate.load(\"recall\")\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    precision = metric_precision.compute(predictions=predictions, references=labels, average=\"micro\")[\"precision\"]\n",
    "    recall = metric_recall.compute(predictions=predictions, references=labels, average=\"micro\")[\"recall\"]\n",
    "    f1 = metric_f1.compute(predictions=predictions, references=labels, average=\"micro\")[\"f1\"]\n",
    "    accuracy = metric_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241db750",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training samples: {len(tokenized_ft)}\")\n",
    "print(f\"Batches per epoch: {len(tokenized_ft) / 8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e6f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ft,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cff8b0",
   "metadata": {},
   "source": [
    "evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get predictions\n",
    "results = trainer.predict(tokenized_test)\n",
    "predictions = np.argmax(results.predictions, axis=-1)\n",
    "\n",
    "# Add to dataframe\n",
    "df_test['prediction'] = predictions\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mistakes\n",
    "print(\"\\nMisclassified examples:\")\n",
    "mistakes = df_test[df_test['labels'] != df_test['prediction']]\n",
    "\n",
    "first_mistake = mistakes.iloc[0]\n",
    "print(f\"Text: {first_mistake['text']}\")\n",
    "print(f\"True: {first_mistake['labels']}\")\n",
    "print(f\"Predicted: {first_mistake['prediction']}\")\n",
    "# note that this is not actually a mistake!\n",
    "# There are some minor errors in the labelled dataset.\n",
    "\n",
    "second_mistake = mistakes.iloc[1]\n",
    "print(f\"Text: {second_mistake['text']}\")\n",
    "print(f\"True: {second_mistake['labels']}\")\n",
    "print(f\"Predicted: {second_mistake['prediction']}\")\n",
    "\n",
    "print(f\"Accuracy: {1-len(mistakes)/len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb408642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(df_test['labels'], df_test['prediction'])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e58ac",
   "metadata": {},
   "source": [
    "Run on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b439e935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saUsX_uimxRlCVr67Z4Jig</td>\n",
       "      <td>8g_iMtfSiwikVnbP2etR0A</td>\n",
       "      <td>YjUWPpI6HXG530lwP-fb2A</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family diner. Had the buffet. Eclectic assortm...</td>\n",
       "      <td>2014-02-05 20:30:30</td>\n",
       "      <td>tucson</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AqPFMleE6RsU23_auESxiA</td>\n",
       "      <td>_7bHUi9Uuf5__HHc_Q8guQ</td>\n",
       "      <td>kxX2SOes4o-D3ZQBkiMRfA</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow!  Yummy, different,  delicious.   Our favo...</td>\n",
       "      <td>2015-01-04 00:01:03</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sx8TMOWLNuJBWer-0pcmoA</td>\n",
       "      <td>bcjbaE6dDog4jkNY91ncLQ</td>\n",
       "      <td>e4Vwtrqf-wpJfwesgvdgxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cute interior and owner (?) gave us tour of up...</td>\n",
       "      <td>2017-01-14 20:54:15</td>\n",
       "      <td>new orleans</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JrIxlS1TzJ-iCu79ul40cQ</td>\n",
       "      <td>eUta8W_HdHMXPzLBBZhL1A</td>\n",
       "      <td>04UD14gamNjLY0IDYVhHJg</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>I am a long term frequent customer of this est...</td>\n",
       "      <td>2015-09-23 23:10:31</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pUycOfUwM8vqX7KjRRhUEA</td>\n",
       "      <td>59MxRhNVhU9MYndMkz0wtw</td>\n",
       "      <td>gebiRewfieSdtt17PTW6Zg</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Had a party of 6 here for hibachi. Our waitres...</td>\n",
       "      <td>2016-07-25 07:31:06</td>\n",
       "      <td>santa barbara</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
       "1  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
       "2  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
       "3  JrIxlS1TzJ-iCu79ul40cQ  eUta8W_HdHMXPzLBBZhL1A  04UD14gamNjLY0IDYVhHJg   \n",
       "4  pUycOfUwM8vqX7KjRRhUEA  59MxRhNVhU9MYndMkz0wtw  gebiRewfieSdtt17PTW6Zg   \n",
       "\n",
       "   stars  useful  funny  cool  \\\n",
       "0      3       0      0     0   \n",
       "1      5       1      0     1   \n",
       "2      4       1      0     1   \n",
       "3      1       1      2     1   \n",
       "4      3       0      0     0   \n",
       "\n",
       "                                                text                 date  \\\n",
       "0  Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30   \n",
       "1  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03   \n",
       "2  Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15   \n",
       "3  I am a long term frequent customer of this est...  2015-09-23 23:10:31   \n",
       "4  Had a party of 6 here for hibachi. Our waitres...  2016-07-25 07:31:06   \n",
       "\n",
       "            city  year  \n",
       "0         tucson  2014  \n",
       "1   philadelphia  2015  \n",
       "2    new orleans  2017  \n",
       "3   philadelphia  2015  \n",
       "4  santa barbara  2016  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "all_reviews = pd.read_csv(\"./all_restaurant_bar_reviews.csv\")\n",
    "all_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for classifying\n",
    "review_df = all_reviews[['review_id', 'text']]\n",
    "\n",
    "# Split into 5 chunks\n",
    "n_chunks = 5\n",
    "chunk_size = len(review_df) // n_chunks\n",
    "remainder = len(review_df) % n_chunks\n",
    "\n",
    "chunks = []\n",
    "start_idx = 0\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    # Add one extra row to the first 'remainder' chunks to handle uneven division\n",
    "    current_chunk_size = chunk_size + (1 if i < remainder else 0)\n",
    "    end_idx = start_idx + current_chunk_size\n",
    "    \n",
    "    chunk = review_df.iloc[start_idx:end_idx].copy()\n",
    "    chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Chunk {i+1}: {len(chunk)} rows (indices {start_idx} to {end_idx-1})\")\n",
    "    start_idx = end_idx\n",
    "\n",
    "print(f\"\\nTotal rows: {len(review_df)}\")\n",
    "print(f\"Sum of chunks: {sum(len(chunk) for chunk in chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e6df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chunked processing\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "chunk_size = 100000  # Process 100K at a time\n",
    "all_predictions = []\n",
    "\n",
    "for i in tqdm(range(0, len(review_df), chunk_size)):\n",
    "    chunk_df = review_df[i:i+chunk_size]\n",
    "    chunk_tokenized = full_tokenized.select(range(i, min(i + chunk_size, len(full_tokenized))))\n",
    "\n",
    "    start = time.time()\n",
    "    chunk_results = trainer.predict(chunk_tokenized)\n",
    "    chunk_predictions = np.argmax(chunk_results.predictions, axis=-1)\n",
    "    all_predictions.extend(chunk_predictions)\n",
    "    \n",
    "    print(f\"Chunk {i//chunk_size + 1}: {len(chunk_df)} samples in {time.time()-start:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0f2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = review_df.copy()\n",
    "output_df['live_music'] = all_predictions\n",
    "output_df.to_csv('reviews_with_live_music.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
